/*
 *
 */

#include <stdlib.h>
#include <assert.h>
//#include <omp.h>
#include "config.h"
#include "np_helper/np_helper.h"
#include "vhf/fblas.h"

void CCunpack_tril(double *tril, double *mat, int count, int n)
{
        NPdunpack_tril_2d(count, n, tril, mat, HERMITIAN);
}

void CCpack_tril(double *tril, double *mat, int count, int n)
{
        NPdpack_tril_2d(count, n, tril, mat);
}

/*
 * a * v1 + b * v2.transpose(0,2,1,3)
 */
void CCmake_0213(double *out, double *v1, double *v2, int count, int m,
                 double a, double b)
{
#pragma omp parallel default(none) \
        shared(count, m, out, v1, v2, a, b)
{
        int i, j, k, l, n;
        size_t d2 = m * m;
        size_t d1 = m * m * m;
        double *pv1, *pv2, *pout;
#pragma omp for schedule (static)
        for (i = 0; i < count; i++) {
                for (n = 0, j = 0; j < m; j++) {
                for (k = 0; k < m; k++) {
                        pout = out + d1*i + d2*j + m*k;
                        pv1  = v1  + d1*i + d2*j + m*k;
                        pv2  = v2  + d1*i + d2*k + m*j;
                        for (l = 0; l < m; l++, n++) {
                                pout[l] = pv1[l] * a + pv2[l] * b;
                        }
        } } }
}
}

/*
 * out = v1 + v2.transpose(0,2,1)
 */
void CCsum021(double *out, double *v1, double *v2, int count, int m)
{
#pragma omp parallel default(none) \
        shared(count, m, out, v1, v2)
{
        int i, j, k, n;
        size_t mm = m * m;
        double *pout, *pv1, *pv2;
#pragma omp for schedule (static)
        for (i = 0; i < count; i++) {
                pout = out + mm * i;
                pv1  = v1  + mm * i;
                pv2  = v2  + mm * i;
                for (n = 0, j = 0; j < m; j++) {
                for (k = 0; k < m; k++, n++) {
                        pout[n] = pv1[n] + pv2[k*m+j];
                } }
        }
}
}

/*
 * g2 = a * v1 + b * v2.transpose(0,2,1)
 */
void CCmake_021(double *out, double *v1, double *v2, int count, int m,
                double a, double b)
{
        if (a == 1 && b == 1) {
                return CCsum021(out, v1, v2, count, m);
        }

#pragma omp parallel default(none) \
        shared(count, m, out, v1, v2, a, b)
{
        int i, j, k, n;
        size_t mm = m * m;
        double *pout, *pv1, *pv2;
#pragma omp for schedule (static)
        for (i = 0; i < count; i++) {
                pout = out + mm * i;
                pv1  = v1  + mm * i;
                pv2  = v2  + mm * i;
                for (n = 0, j = 0; j < m; j++) {
                for (k = 0; k < m; k++, n++) {
                        pout[n] = pv1[n] * a + pv2[k*m+j] * b;
                } }
        }
}
}

/*
 * if matrix B is symmetric for the contraction A_ij B_ij,
 * Tr(AB) ~ A_ii B_ii + (A_ij + A_ji) B_ij where i > j
 * This function extract the A_ii and the lower triangluar part of A_ij + A_ji
 */
void CCprecontract(double *out, double *in, int count, int m, double diagfac)
{
#pragma omp parallel default(none) \
        shared(count, m, in, out, diagfac)
{
        int i, j, k, n;
        size_t mm = m * m;
        size_t m2 = m * (m+1) / 2;
        double *pout, *pin;
#pragma omp for schedule (static)
        for (i = 0; i < count; i++) {
                pout = out + m2 * i;
                pin  = in  + mm * i;
                for (n = 0, j = 0; j < m; j++) {
                        for (k = 0; k < j; k++, n++) {
                                pout[n] = pin[j*m+k] + pin[k*m+j];
                        }
                        pout[n] = pin[j*m+j] * diagfac;
                        n++;
                }
        }
}
}

/*
 * Given shell ID and the index (within the shell) to load, construct ERIs[index,:,:,:],
 * where ERIs are cached in the eri_pool generated by nr_ao2mo(aosym=s4) function
 * eri_pool is a concatenated array
 * ([i0:i1,j0:j1,...],[i0:i1,j1:j2,...],...,[i1:i2,j0:j1,...],...).
 *
 * def load_eri_s4(eri, ao_loc, ish, idx):
 *     di = ao_loc[ish+1] - ao_loc[ish]
 *     nj = ao_loc[ish+1]
 *     ijloc = di * ao_loc
 *     nao_pair = nao * (nao+1) // 2
 *     out = numpy.empty((nj,nao_pair))
 *     for jsh in range(ish):
 *         out[ao_loc[jsh]:ao_loc[jsh+1]] = eri[ijloc[jsh]:ijloc[jsh+1]].reshape(di,-1,nao_pair)[idx]
 *     off = ijloc[ish] + idx*(idx+1)//2
 *     out[ao_loc[ish]:ao_loc[ish]+idx+1] = eri[off:off+idx+1]
 *     out = lib.unpack_tril(out.reshape(-1,nao_pair), out=buf1)
 *     return out
 */
void CCload_eri_s4(double *out, double *eri_pool, int *ao_loc,
                   int ish, int idx, int nao)
{
        size_t nao_pair = nao * (nao + 1) / 2;
        int di = ao_loc[ish+1] - ao_loc[ish];
        int eri_lookup[nao];
        int jsh, j, dj, nsegs, ij0, ij1;
        for (nsegs = 0, jsh = 0; jsh < ish; jsh++) {
                dj = ao_loc[jsh+1] - ao_loc[jsh];
                ij0 = ao_loc[jsh] * di + idx * dj;
                ij1 = ij0 + dj;
                for (j = ij0; j < ij1; j++, nsegs++) {
                        eri_lookup[nsegs] = j;
                }
        }
        // jsh == ish
        ij0 = ao_loc[ish] * di + idx*(idx+1)/2;
        ij1 = ij0 + idx + 1;
        for (j = ij0; j < ij1; j++, nsegs++) {
                eri_lookup[nsegs] = j;
        }

#pragma omp parallel default(none) \
        shared(eri_pool, out, nao, nao_pair, nsegs, eri_lookup)
{
        int i;
        size_t off;
        size_t nao2 = nao * nao;
#pragma omp for schedule (static)
        for (i = 0; i < nsegs; i++) {
                off = eri_lookup[i] * nao_pair;
                NPdunpack_tril(nao, eri_pool+off, out+i*nao2, 1);
        }
}
}
